---
title: "GPU prediction notebook"
author: "Nataniel Moreau"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pacman)
p_load(tidyverse, ggthemes, scales, tidymodels, janitor, 
       magrittr, glmnet, modeldata, 
       baguette, data.table, parallel, xgboost, skimr, scales, caret, leaps, usemodels, stargazer)

```

#### Prep & clean & explore data
```{r}
# Prep & clean & explore data -------------------------------------------------------------------------

laptop_data <- read_csv("laptop_data.csv")

# clean names
laptop_data %<>% clean_names()

# convert to USD from rupees and clean up decimals (conversion will need to be updated manually)
laptop_data %<>% mutate(price = round(price * .012, digits = 0))

set.seed(246810)

# explore data 
skim(laptop_data)

# distribution of prices
laptop_data %>% ggplot(aes(price)) + 
  geom_density(aes(fill = "darkofchild", alpha = .8)) + 
  geom_vline(aes(xintercept = mean(price))) + 
  labs(title = "Distribution of Prices", 
       subtitle = "mean line at $718", 
       x = "Prices (USD)") + 
  theme_clean()+ 
  scale_y_continuous(labels = percent_format())+ 
  theme(legend.position = "none", 
        plot.title = element_text(hjust = .5), 
        plot.subtitle = element_text(hjust = .5, size = 10))

# most common laptop ram
laptop_data %>% ggplot(aes(ram)) + 
  geom_bar(aes(fill = "darkofchild", alpha = .8), show.legend = FALSE) + 
  labs(title = "Most common types of ram")+ 
  theme_clean() + 
  theme(plot.title = element_text(hjust = .5))

# split data into training/testing
laptop_split = laptop_data %>% initial_split(prop = .8)
laptop_train = laptop_split %>% training()
laptop_test = laptop_split %>% testing()

# create cv folds
laptop_cv = laptop_train %>% 
  vfold_cv(v = 5)

# create base recipe for models
laptop_recipe = recipe(price ~ ., data = laptop_train) %>% 
  update_role(x1, new_role = "id variable") %>% 
  step_impute_mode(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors())
  
```

#### Model 1: Sequential stepwise selection
```{r, warning=FALSE}
# Model 1: seq step wise selection      ------------------------------------
set.seed(246810)

#copy training & testing data for this model to prep & bake
laptop_train_1 = laptop_train
laptop_test_1 = laptop_test

# prep recipe
prepped_train_1 = prep(laptop_recipe, training = laptop_train_1)

#bake
laptop_train_baked = bake(prepped_train_1, laptop_train_1)
laptop_test_baked = bake(prepped_train_1, laptop_test_1)

# for cross validation
train.control <- trainControl(method = "cv", number = 5)

#tune model specification
seq_step_wise = train(price ~ . -x1, data = laptop_train, 
                      method = "leapSeq", 
                      tuneGrid = data.frame(nvmax = 1:10), 
                      trControl = train.control)

# find best model: best includes all variables 
seq_step_wise$bestTune
seq_step_wise$results

# run best model w/ 10 vars
step_wise_best = lm(price ~ . -x1, data = laptop_train_baked)

# predict and calc rmse with best model
predict_simple_lm = predict(step_wise_best, newdata = laptop_test_baked)

# put in dataset to calc rmse
simple_reg_df = data.frame(real = laptop_test_baked$price, 
                   mod1 = predict_simple_lm)

# impute mean for NA's
simple_reg_df$mod1[is.na(simple_reg_df$mod1)] = mean(simple_reg_df$mod1, na.rm = T)

#compute rmse
simple_rmse = sqrt(mean((simple_reg_df$real - simple_reg_df$mod1)^2))

```

#### Model 2: Lasso
```{r, warning=FALSE, message=FALSE}
# define model 
laptop_lasso = 
  linear_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")

# define workflow
lasso_workflow = workflow() %>% 
  add_model(laptop_lasso) %>% 
  add_recipe(laptop_recipe)

set.seed(246810)

# first fit
fit_lasso = lasso_workflow %>% 
  tune_grid(laptop_cv, grid = expand_grid(penalty = seq(.1,5,by = .1)), 
            metrics = metric_set(rmse, mae))

# select best models
show_best(fit_lasso, metric = "rmse")
collect_metrics(fit_lasso)
      
# predict onto test data with last_fit()
final_lasso = 
  lasso_workflow %>% 
  finalize_workflow(select_best(fit_lasso, metric = "rmse"))

final_fit_lasso = 
  last_fit(final_lasso, laptop_split)

collect_metrics(final_fit_lasso)
```


#### Model 3: Elasticnet 
```{r, warning=FALSE,message=FALSE}

# define model 
laptop_enet = 
  linear_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet")

# define workflow
enet_workflow = workflow() %>% 
  add_model(laptop_enet) %>% 
  add_recipe(laptop_recipe)

set.seed(246810)

# first fit
fit_enet = enet_workflow %>% 
  tune_grid(laptop_cv, grid = expand_grid(mixture = seq(0,1, by = .1),
                                          penalty = seq(1,5, by = .1)), 
            metrics = metric_set(rmse, mae))

# select best models
show_best(fit_enet, metric = "rmse")
collect_metrics(fit_enet)

# predict onto test data with last_fit()
final_enet = 
  enet_workflow %>% 
  finalize_workflow(select_best(fit_enet, metric = "rmse"))

final_fit_enet = 
  last_fit(final_enet, laptop_split)

collect_metrics(final_fit_enet)
```

#### Model 4: Boosted Trees
```{r, warning=FALSE, message=FALSE}
#use_xgboost()
laptop_boost = boost_tree(mode = "regression", 
                       mtry = 10, 
                       min_n = 8, 
                       trees = tune(), 
                       tree_depth = tune(),
                       learn_rate = tune()) %>% 
  set_engine("xgboost")

#define workflow 
boost_wkfl = workflow() %>%
  add_model(laptop_boost) %>% 
  add_recipe(laptop_recipe)

set.seed(246810)
#fit using cv
boost_fit = boost_wkfl %>% 
  tune_grid(laptop_cv, 
             grid = expand_grid(tree_depth = seq(1,10, by = 1), 
                                learn_rate = seq(.01,.1, by = .01), 
                                trees = seq(50,100,10)), 
             metrics = metric_set(rmse, mae))

#best models
show_best(boost_fit, metric = "rmse")
collect_metrics(boost_fit)

# predict onto test data with last_fit()
final_boost = 
  boost_wkfl %>% 
  finalize_workflow(select_best(boost_fit, metric = "rmse"))

final_fit_boost = 
  last_fit(final_boost, laptop_split)

collect_metrics(final_fit_boost)

```

